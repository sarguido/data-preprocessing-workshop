{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing data for machine learning\n",
    "\n",
    "### What is standardization?\n",
    "\n",
    "Standardization is a preprocessing method used to transform continuous data to make it look normally distributed. In scikit-learn, this is often a necessary step, because many models assume that the data you are training on is normally distributed, and if it isn't, you risk biasing your model. You can standardize your data in different ways, but in this workshop we're going to talk about two methods: log normalization and scaling. \n",
    "\n",
    "There are a few different scenarios in which you want to standardize your data. First, if you're working with any kind of model that uses a linear distance metric or operates in a linear space like k-nearest neighbors, linear regression, or k-means clustering, the model is assuming that the data and features you're giving it are related in a linear fashion, or can be measured with a linear distance metric. There are a number of models that deal with nonlinear spaces, but for those models that are in a linear space, the data must also be in that space.\n",
    "\n",
    "The case when a feature or features in your dataset have high variance is similar. This could bias a model that assumes the data is normally distributed. If a feature in your dataset has a variance that's an order of magnitude or more greater than the other features, this could impact the model's ability to learn from other features in the dataset. Also, modeling a dataset that contains continuous features that are on different scales is another scenario to watch out for. For example, consider a dataset that contains a column related to height and another related to weight. In order to compare these features, they must be in the same linear space, and therefore must be standardized in some way.\n",
    "\n",
    "### Log normalization\n",
    "\n",
    "Log normalization is a method for standardizing your data that can be useful when you have a particular column with high variance. Log normalization applies a log transformation to your values, which transforms your values onto a scale that approximates normality, an assumption about data that a lot of machine learning models make. The method of log normalization we're going to work with in Python takes the natural log of each number, which is simply the exponent you would raise above the mathematical constant _e_ (approximately equal to 2.718) to get that number.  \n",
    "\n",
    "Log normalization is a good strategy when you care about relative changes in a linear model, when you still want to capture the magnitude of change, and when you want to keep everything in the positive space. It's a nice way to minimize the variance of a column and make it comparable to other columns for modeling.\n",
    "\n",
    "Let's see how this works in Python. Here we have a small dataset where one of the columns, `col2`, has high variance. We can check the variance across a set of data using the `.var()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "var_df = pd.DataFrame({\"col1\": [1.0, 1.2, 0.75, 1.6], \n",
    "                       \"col2\": [3.0, 45.5, 28.0, 100.0]})\n",
    "\n",
    "var_df.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying log normalization to data in Python is fairly straightforward. We can use the `np.log()` function from NumPy to log normalize `col2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "var_df[\"col2_log\"] = ____(var_df[\"col2\"])\n",
    "var_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the variance of both `col1` and the `col2_log`, we can see that the variances are much closer together now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "____(var_df[[\"col1\", \"col2_log\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling for feature comparison\n",
    "\n",
    "Scaling is a method of standardization that's most useful when you're working with a dataset that contains continuous features that are on different scales, and you're using a model that operates in some sort of linear space (like linear regression or k-nearest neighbors). Feature scaling transforms the features in your dataset so they have a mean of zero and a variance of one. This will make it easier to linearly compare features. This is a requirement for many models in scikit-learn.\n",
    "\n",
    "Let's take a look at another dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict = {\"col1\": [1.0, 1.2, 0.75, 1.6],\n",
    "              \"col2\": [48.0, 45.5, 46.2, 50.0],\n",
    "              \"col3\": [100.0, 101.3, 103.5, 104.1]}\n",
    "\n",
    "scale_df = pd.DataFrame(scale_dict)\n",
    "\n",
    "scale_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each column, we have numbers that are relatively close within the column, but not across columns. If we look at the variance, it's relatively low across columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_df.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better model this data, scaling would be a good choice here.\n",
    "\n",
    "Scikit-learn has a variety of scaling methods, but we're only going to focus on the `StandardScaler()` method. This method works by removing the mean and scaling each feature to have unit variance. There's a simpler scale function in scikit-learn, but the benefit of using `StandardScaler()` is that you can apply the same transformation on other data, like a test set, or new data that's part of the same set, for example, without having to rescale everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the standard scaler method, we can apply the `fit_transform()` function on the DataFrame, setting the `columns` parameter to `scale_df.columns`. Here we're keeping it as a DataFrame to see the results more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = pd.DataFrame(scaler.____(scale_df), columns=____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the DataFrame, it has now been scaled, and the variance is now equivalent across columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_scaled)\n",
    "print(\"\\n\")\n",
    "print(df_scaled.____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!\n",
    "\n",
    "Let's try out two situations where you'd want to use these standardization techniques on a dataset, beginning with log normalization.\n",
    "\n",
    "First, read in the csv file. Then take a look at the feature variance in the `wine_types` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_string = \"../../datasets/\"\n",
    "wine_types = ____(dir_string + \"wine_types.csv\")\n",
    "\n",
    "wine_types.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `Proline` column has much higher variance than anything else in the dataset. This column would be a good candidate for log normalization. Let's apply the `np.log()` function to the `Proline` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_types[\"Proline_log\"] = ____\n",
    "\n",
    "wine_types[\"Proline_log\"].____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the variance has been reduced, which will make feature comparisons easier.\n",
    "\n",
    "Next, apply scaling to the `wine_types` dataset as well. Let's say that we want to use the `Ash`, `Alcalinity of ash`, and `Magnesium` columns to train a linear model, but it's possible that these columns are all measured in different ways. \n",
    "\n",
    "Taking a quick look at these columns with the `.describe()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_types[[\"Ash\", \"Alcalinity of ash\", \"Magnesium\"]].____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these three columns are on different scalesâ€”the means, mins, and maxes are wildly different. Since we can see that these columns are all on different scales, let's standardize them in a way that allows for use in a linear model.\n",
    "\n",
    "First we'll create a new `StandardScaler()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_scaler = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll apply it to the subset of the `wine_types` dataset using `fit_transform()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_subset = wine_types[[\"Ash\", \"Alcalinity of ash\", \"Magnesium\"]]\n",
    "\n",
    "wine_subset_scaled = wine_scaler.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's take a look at the scaled data and its variance by transforming it back into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_subset_scaled_df = pd.DataFrame(wine_subset_scaled, columns=wine_subset.columns) \n",
    "\n",
    "print(wine_subset_scaled_df.head())\n",
    "print(\"\\n\")\n",
    "print(wine_subset_scaled_df.____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been scaled and the variance is equal across columns.\n",
    "\n",
    "### knn refresher\n",
    "\n",
    "Let's do a quick refresher on k-nearest neighbors and scikit-learn in general, which we'll be using throughout the workshop. \n",
    "\n",
    "K-nearest neighbors is a model that classifies data based on its distance to training set data. A new data point is assigned a label based on the class that the majority of surrounding data points belongs to, using a distance metric like the Euclidean distance metric. \n",
    "\n",
    "Let's walk through an example of k-nearest neighbors and the scikit-learn workflow. First, let's generate a toy dataset using scikit-learn's `make_classification()` method, which generates a normally-distributed dataset you can use for classification problems. There are a number of ways you can customize the generated dataset, but we're going to keep it simple here. \n",
    "\n",
    "The parameters filled in below are:\n",
    "- `n_samples`: the number of samples (rows) in the dataset. The default is 100; here it's increased to 1000.\n",
    "- `n_features`: the number of features (columns) in the dataset. Instead of the default 20, let's keep it very simple at 3 features.\n",
    "- `n_classes`: the number of class labels. We'll keep it at the default 2. \n",
    "- `n_redundant`: the number of redundant featuresâ€”features that aren't informative for classificationâ€”which is useful if you want to test out dimensionality reduction methods, for example. We're not doing that here, so we'll set it to 0.\n",
    "\n",
    "The function generates both the X and y sets, so we'll store those sets in the `X_gen` and `y_gen` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_gen, y_gen = make_classification(n_samples=1000, n_features=3, n_classes=2, n_redundant=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, let's build our model. We'll need both `train_test_split`, to split up our dataset before training, and `KNeighborsClassifier()`, to train the k-nearest neighbors model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split up our data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've done that, we can create our `KNeighborsClassifier()` model, and then use `.fit()` to fit the model to our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = ____\n",
    "knn.____(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step we want to take is to check the accuracy of our classifier. There are a multitude of ways you can validate the accuracy of your mode, but we'll just use `KNeighborsClassifier()`'s built in `.score()` method, which takes your `X` test data, predicts its `y` labels, and then compares the predicted `y` to the true values of `y` you pass in as the second parameter. The output is the percentage in the test `X` that received an accurate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.____(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!\n",
    "\n",
    "In this section, we're going to build a k-nearest neighbors model on both non-scaled and scaled data, to give you a better idea of how that impacts model performance.\n",
    "\n",
    "#### Wine_types modeling: non-scaled data\n",
    "\n",
    "Let's first take a look at the accuracy of a k-nearest neighbors model on the `wine_types` dataset without standardizing the data. We're going to use the features in this dataset to predict the `Type` of wine, of which there are three labels: 1, 2, and 3.\n",
    "\n",
    "Let's break out `Type` into its own `y_wine` dataset as our predicted class, and we'll keep the rest of the features in `X_wine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wine = wine_types.drop(\"Type\", axis=1)\n",
    "y_wine = wine_types[\"Type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already previously imported `train_test_split` so let's split up our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a new `KNeighborsClassifier()` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_wine_unscaled = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can fit the model to our training set and then take a look at its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_wine_unscaled.____\n",
    "\n",
    "knn_wine_unscaled.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy isn't terrible, but it's possible that we can improve our model by scaling our data!\n",
    "\n",
    "#### Wine_types modeling: scaled data\n",
    "\n",
    "The accuracy score on the unscaled `wine_types` dataset was decent, but we can likely do better if we scale the dataset. The process is mostly the same as the previous exercise, with the added step of scaling the data.\n",
    "\n",
    "Let's apply the `StandardScaler` method to our `wine_types` feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_scaler = ____\n",
    "\n",
    "X_scaled = wine_scaler.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we'll split up the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then fit the `KNeighborsClassifier` model to the data and take a look at the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_wine_scaled = ____\n",
    "knn_wine_scaled.____\n",
    "\n",
    "knn_wine_scaled.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there's been a dramatic increase in the accuracy of our model, simply by scaling our dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
