{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Steps in Data Preprocessing\n",
    "\n",
    "### What is data preprocessing?\n",
    "\n",
    "Data preprocessing is the step in between cleaning up/exploring your data and building a machine learning model. The lines are often blurred; you may need to do some cleaning during preprocessing, and you might need to go back to preprocessing after modeling to tweak your feature set. Once you understand your dataset, you'll probably have some idea about how you want to model your data. Machine learning models in Python require numerical input, so if your dataset has categorical variables, you'll need to transform them. \n",
    "\n",
    "### Pandas refresher\n",
    "\n",
    "To start, let's do a quick Pandas refresher (you should already be familiar with Pandas for this workshop). Pandas is a data processing library in Python that is incredibly useful for preprocessing data and is one of my all-time favorite Python libraries. \n",
    "\n",
    "Reading a dataset into pandas is pretty straightforward, and it's possible to read in data in a variety of formats. To read in a JSON file, we can use `read_json`. To check our results, let's look at the `head()` of the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# The path to the data directory. Easier than typing it out every time!\n",
    "dir_string = \"../../datasets/\"\n",
    "\n",
    "hiking = pd.____(dir_string + \"hiking.json\")\n",
    "hiking.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll read in a CSV of data on different wine attributes using `read_csv`, and once again check our input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.____(dir_string + \"wine_types.csv\")\n",
    "wine.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to be able to generate a list of the features present in your dataset. To see the columns in a datasets, use the `.columns` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiking.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to see the data types of the columns in your dataset, use `.dtypes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiking.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most useful basic features of pandas is the ability to quickly output characteristics of your dataset like the mean, standard deviation, and quartile values. To see these values, you can `.describe()` your dataset. Note that this method will only work on continuous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data\n",
    "\n",
    "Now that we've refreshed ourselves on the basics of pandas, let's cover some ways for how to deal with missing data. There are a variety of techniques you can use, but here we'll focus on a few ways to remove missing data on a simple dataset, and then try out a few techniques on a couple of \"real world\" datasets in the next step.\n",
    "\n",
    "Let's create a simple toy dataset with some missing data. Here we'll encode some `NaN` values into the dataset using NumPy and easily create a DataFrame with pandas's `DataFrame` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_dict = {\"A\": [6, 4, 9, np.NaN, 3],\n",
    "           \"B\": [np.NaN, 8 , np.NaN, 2, 1],\n",
    "           \"C\": [9, 3, np.NaN, np.NaN, 9]}\n",
    "\n",
    "df = pd.DataFrame(df_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can drop all rows with missing data in your dataset with `.dropna()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can drop specific rows by passing index labels to the drop function, which defaults to dropping rows, using the `.drop()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.____([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually you'll want to focus on dropping a particular column, especially if all or most of its values are missing. If you want to drop columns, you can pass either a single column or a list of columns into the `.drop()` method. You'll also have to specify `axis=1` in order to drop columns instead of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"A\", ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to drop rows where data is missing in a particular column? We can do this with the help of boolean indexing, which is a way to filter a DataFrame through a conditional statement. Here we'll select all rows from the dataset where column C is equal to 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"C\"] ____]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use boolean indexing in combination with the `.notnull()` method to select all rows in a dataset where a certain column or set of columns is not null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"C\"].____]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!\n",
    "\n",
    "We have a dataset comprised of volunteer information from New York City. We can think of each column in the dataset as a feature. This dataset has features pertaining to the time of the volunteering event, the category, the location, the organization, and so on. \n",
    "\n",
    "The dataset has a number of features, but let's say that we want to get rid of features that have no more than 65 missing values. \n",
    "\n",
    "First, let's read in the dataset and take a look at its `head()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer = pd.____(dir_string + \"volunteer.csv\")\n",
    "volunteer.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the shape of the dataset as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have 665 rows and 35 columns. We're going to drop those columns that have more than 65 missing values. We can do this with the `.dropna()` method by setting two parameters: `axis=1` to specify we want to drop columns, which you've already done above, and `thresh=600` to specify that we want to keep columns with at least 600 non-null values. \n",
    "\n",
    "Fill in the parameters below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer_drop_columns = volunteer.____\n",
    "volunteer_drop_columns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we've dropped 13 columns that had at least 65 missing values (and 600 non-null values).\n",
    "\n",
    "Taking a look at this dataset again, we want to drop rows where the `category_desc` column values are missing. We're going to do this using boolean indexing, by checking to see if we have any null values, and then filtering the dataset so that we only have rows with those values.\n",
    "\n",
    "First, we can check how many values are missing in a column using `.isnull()` to get the null values and `.sum()` to get the total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer[\"category_desc\"].____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have 48 null values. We can once again use `.notnull()` to subset the DataFrame using boolean indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer_subset = volunteer[volunteer[\"category_desc\"].____]\n",
    "\n",
    "volunteer_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we've retained all 35 features and that we've dropped the 48 rows where the column `category_desc` didn't have a value.\n",
    "\n",
    "### Exploring data types\n",
    "\n",
    "One of the next steps of preprocessing is to think about the types that are present in your dataset, because you'll likely have to transform some of these columns to other types later on. Let's take a deeper look at types as well as how to convert column types in your dataset.\n",
    "\n",
    "You can check the types of a DataFrame by using the `.dtypes` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a list of some of the most common datatypes:\n",
    "- `object`: pandas uses this type to refer to a column that consists of string values or is of mixed types. \n",
    "- `int64`: the equivalent of the Python integer type. The 64 simply refers to memory allotted for storing the values. \n",
    "- `float64`: the equivalent of the Python float type. \n",
    "- `datetime64` (or the `timedelta` type): dates in pandas can be stored as a special type that makes time operations easier or set as an index.\n",
    "\n",
    "Sometimes, you'll start working with a dataset that has an incorrect column type: maybe a numerical column was written out into a csv as a string, and when you try to work with that column, numerical operations won't work. \n",
    "\n",
    "Let's convert some columns, first using a small example dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_types = pd.DataFrame({\"A\": [1, 2, 3], \n",
    "                         \"B\": [\"string1\", \"string2\", \"string3\"], \n",
    "                         \"C\": [\"1.0\", \"2.0\", \"3.0\"]})\n",
    "\n",
    "print(df_types)\n",
    "print(\"\\n\")\n",
    "print(df_types.____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, column C has the `dtype` of `object`. However, looking at it, it's clear that column C should be the type `float`. Converting a column type is pretty straightforward using `.astype()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_types[\"C\"] = df_types[\"C\"].____\n",
    "\n",
    "df_types.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now column C is the correct type.\n",
    "\n",
    "### Class imbalance and distribution\n",
    "\n",
    "One of the most necessary steps for preprocessing, which you should be familiar with if you've taken other courses on Python and machine learning, is splitting up your data into training and test sets. We do this to avoid the issue of overfitting. If we train a model on our entire set of data, we won't have any way to test and validate our model because the model will essentially know the dataset by heart. Holding out a test set allows us to preserve some data the model hasn't seen yet.\n",
    "\n",
    "Splitting data into training and test sets is straightfoward with scikit-learn's `train_test_split()` function. This example, on a simple dataset, should be familiar already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split_df = pd.DataFrame({\"X\": [1, 2, 3, 4, 5, 6], \"y\": [\"y\", \"y\", \"n\", \"n\", \"y\", \"n\"]})\n",
    "\n",
    "print(split_df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the split sets using `pd.concat()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set\")\n",
    "print(pd.concat([X_train, y_train], axis=1))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Test set\")\n",
    "print(pd.concat([X_test, y_test], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method works well, but there are going to be scenarios where your `y` values—the values you're potentially predicting in a model—aren't as evenly distributed in your dataset. For example, if 80% of your dataset falls into the `y` class, and 20% falls into the `n` class, it's possible that when you run `train_test_split`, this distribution isn't represented in your test set. Possibly you'll end up with no `n` values in the test set, which will severely bias your model.\n",
    "\n",
    "To avoid this scenario, we can use a technique called stratified sampling, which is a way of sampling that takes into account the distribution of classes or features in your dataset. With stratified sampling, we can preserve that 80%/20% distribution across both our training and test sets, which means we're training and testing our model in similar circumstances.\n",
    "\n",
    "First, let's read in the example dataset and look at the distribution for our `Y` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_df = pd.read_csv(dir_string + \"stratified_df.csv\")\n",
    "\n",
    "print(stratified_df.head())\n",
    "print(\"\\n\")\n",
    "print(stratified_df[\"Y\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we have 80% of our data with a `y` label, and 20% with `n`. In order to preserve this distribution when splitting up the dataset, we can use the `stratify=` parameter in the `train_test_split` function. All we need to do is pass in the values we want to stratify the split by, which in this case is the `Y` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(stratified_df[\"X\"], stratified_df[\"Y\"], ____)\n",
    "\n",
    "print(y_train.value_counts())\n",
    "print(\"\\n\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the distribution of classes is in accordance with the original y class distribution. \n",
    "\n",
    "### Your turn!\n",
    "\n",
    "Let's say that, using the `volunteer` dataset, we're thinking about trying to predict the `category_desc` variable using the other features in the dataset. First, though, we need to know what the class distribution (and imbalance) is for that particular label. We're going to use the `volunteer` dataset without nulls in the `category_desc` column, which we created earlier in the notebook.\n",
    "\n",
    "First, let's check the distribution using `.value_counts()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer_subset[\"category_desc\"].____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across the different categories, we have an uneven distribution of classes. If we were building a model to predict these classes, we'd want to make sure we retain this distribution across the training and test sets. So, let's prepare the `volunteer` dataset for stratified sampling. Since the `volunteer` dataset has so many features, we're going to break out the `category_desc` to use as our predicted class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer_X = volunteer_subset.drop(\"category_desc\", axis=1)\n",
    "volunteer_y = volunteer_subset[\"category_desc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass the data into `train_test_split()` and set the `stratify=` parameter to the `category_desc` distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = ____\n",
    "\n",
    "print(y_train.value_counts())\n",
    "print(\"\\n\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both sets have the original distribution preserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
