{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing data for machine learning\n",
    "\n",
    "### What is standardization?\n",
    "\n",
    "Blah blah\n",
    "\n",
    "### Log normalization\n",
    "\n",
    "Log normalization is a method for standardizing your data that can be useful when you have a particular column with high variance. Log normalization applies a log transformation to your values, which transforms your values onto a scale that approximates normality, an assumption about data that a lot of machine learning models make. The method of log normalization we're going to work with in Python takes the natural log of each number, which is simply the exponent you would raise above the mathematical constant _e_ (approximately equal to 2.718) to get that number.  \n",
    "\n",
    "Log normalization is a good strategy when you care about relative changes in a linear model, when you still want to capture the magnitude of change, and when you want to keep everything in the positive space. It's a nice way to minimize the variance of a column and make it comparable to other columns for modeling.\n",
    "\n",
    "Let's see how this works in Python. Here we have a small dataset where one of the columns, `col2`, has high variance. We can check the variance across a set of data using the `.var()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "col1       0.128958\n",
       "col2    1691.729167\n",
       "dtype: float64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "var_df = pd.DataFrame({\"col1\": [1.0, 1.2, 0.75, 1.6], \n",
    "                       \"col2\": [3.0, 45.5, 28.0, 100.0]})\n",
    "\n",
    "var_df.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying log normalization to data in Python is fairly straightforward. We can use the `np.log()` function from NumPy to log normalize `col2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col2_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.20</td>\n",
       "      <td>45.5</td>\n",
       "      <td>3.817712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.332205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.60</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.605170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1   col2  col2_log\n",
       "0  1.00    3.0  1.098612\n",
       "1  1.20   45.5  3.817712\n",
       "2  0.75   28.0  3.332205\n",
       "3  1.60  100.0  4.605170"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "var_df[\"col2_log\"] = np.log(var_df[\"col2\"])\n",
    "var_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the variance of both `col1` and the `col2_log`, we can see that the variances are much closer together now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "col1        0.096719\n",
       "col2_log    1.697165\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(var_df[[\"col1\", \"col2_log\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling for feature comparison\n",
    "\n",
    "Scaling is a method of standardization that's most useful when you're working with a dataset that contains continuous features that are on different scales, and you're using a model that operates in some sort of linear space (like linear regression or k-nearest neighbors). Feature scaling transforms the features in your dataset so they have a mean of zero and a variance of one. This will make it easier to linearly compare features. This is a requirement for many models in scikit-learn.\n",
    "\n",
    "Let's take a look at another dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>48.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.20</td>\n",
       "      <td>45.5</td>\n",
       "      <td>101.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>46.2</td>\n",
       "      <td>103.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.60</td>\n",
       "      <td>50.0</td>\n",
       "      <td>104.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2   col3\n",
       "0  1.00  48.0  100.0\n",
       "1  1.20  45.5  101.3\n",
       "2  0.75  46.2  103.5\n",
       "3  1.60  50.0  104.1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_dict = {\"col1\": [1.0, 1.2, 0.75, 1.6],\n",
    "              \"col2\": [48.0, 45.5, 46.2, 50.0],\n",
    "              \"col3\": [100.0, 101.3, 103.5, 104.1]}\n",
    "\n",
    "scale_df = pd.DataFrame(scale_dict)\n",
    "\n",
    "scale_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each column, we have numbers that are relatively close within the column, but not across columns. If we look at the variance, it's relatively low across columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "col1    0.128958\n",
       "col2    4.055833\n",
       "col3    3.649167\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_df.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better model this data, scaling would be a good choice here.\n",
    "\n",
    "Scikit-learn has a variety of scaling methods, but we're only going to focus on the `StandardScaler()` method. This method works by removing the mean and scaling each feature to have unit variance. There's a simpler scale function in scikit-learn, but the benefit of using `StandardScaler()` is that you can apply the same transformation on other data, like a test set, or new data that's part of the same set, for example, without having to rescale everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the standard scaler method, we can apply the fit transform function on the DataFrame. Here we're keeping it as a DataFrame to see the results more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = pd.DataFrame(scaler.fit_transform(scale_df), columns=scale_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the DataFrame, it has now been scaled, and the variance is now equivalent across columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       col1      col2      col3\n",
      "0 -0.442127  0.329683 -1.344939\n",
      "1  0.200967 -1.103723 -0.559132\n",
      "2 -1.245995 -0.702369  0.770695\n",
      "3  1.487156  1.476409  1.133375\n",
      "\n",
      "\n",
      "col1    1.333333\n",
      "col2    1.333333\n",
      "col3    1.333333\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_scaled)\n",
    "print(\"\\n\")\n",
    "print(df_scaled.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization and modeling\n",
    "\n",
    "Let's try out two situations where you'd want to use these standardization techniques on a dataset, beginning with log normalization. We'll then do some modeling with non-standardized and standardized data, to compare the results.\n",
    "\n",
    "If we take a look at the feature variance in the `wine_types` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type                                0.600679\n",
       "Alcohol                             0.659062\n",
       "Malic acid                          1.248015\n",
       "Ash                                 0.075265\n",
       "Alcalinity of ash                  11.152686\n",
       "Magnesium                         203.989335\n",
       "Total phenols                       0.391690\n",
       "Flavanoids                          0.997719\n",
       "Nonflavanoid phenols                0.015489\n",
       "Proanthocyanins                     0.327595\n",
       "Color intensity                     5.374449\n",
       "Hue                                 0.052245\n",
       "OD280/OD315 of diluted wines        0.504086\n",
       "Proline                         99166.717355\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_string = \"../../datasets/\"\n",
    "wine_types = pd.read_csv(dir_string + \"wine_types.csv\")\n",
    "\n",
    "wine_types.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `Proline` column has much higher variance than anything else in the dataset. This column would be a good candidate for log normalization. If we apply the same `np.log()` function we used before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17231366191842018"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_types[\"Proline_log\"] = np.log(wine_types[\"Proline\"])\n",
    "\n",
    "wine_types[\"Proline_log\"].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the variance has been reduced, which will make feature comparisons easier.\n",
    "\n",
    "Finally, let's apply scaling to the `wine_types` dataset as well. Let's say that we want to use the `Ash`, `Alcalinity of ash`, and `Magnesium` columns to train a linear model, but it's possible that these columns are all measured in different ways. \n",
    "\n",
    "Taking a quick look at these columns with the `.describe()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.366517</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>99.741573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.274344</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>14.282484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.360000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.210000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.360000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.557500</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>107.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.230000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>162.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Ash  Alcalinity of ash   Magnesium\n",
       "count  178.000000         178.000000  178.000000\n",
       "mean     2.366517          19.494944   99.741573\n",
       "std      0.274344           3.339564   14.282484\n",
       "min      1.360000          10.600000   70.000000\n",
       "25%      2.210000          17.200000   88.000000\n",
       "50%      2.360000          19.500000   98.000000\n",
       "75%      2.557500          21.500000  107.000000\n",
       "max      3.230000          30.000000  162.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_types[[\"Ash\", \"Alcalinity of ash\", \"Magnesium\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these three columns are on different scales—the means, mins, and maxes are wildly different. Since we can see that these columns are all on different scales, let's standardize them in a way that allows for use in a linear model.\n",
    "\n",
    "First we'll create a new `StandardScaler()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll apply it to the subset of the `wine_types` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_subset = wine_types[[\"Ash\", \"Alcalinity of ash\", \"Magnesium\"]]\n",
    "\n",
    "wine_subset_scaled = wine_scaler.fit_transform(wine_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's take a look at the scaled data and its variance by transforming it back into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Ash  Alcalinity of ash  Magnesium\n",
      "0  0.232053          -1.169593   1.913905\n",
      "1 -0.827996          -2.490847   0.018145\n",
      "2  1.109334          -0.268738   0.088358\n",
      "3  0.487926          -0.809251   0.930918\n",
      "4  1.840403           0.451946   1.281985\n",
      "\n",
      "\n",
      "Ash                  1.00565\n",
      "Alcalinity of ash    1.00565\n",
      "Magnesium            1.00565\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "wine_subset_scaled_df = pd.DataFrame(wine_subset_scaled, columns=wine_subset.columns) \n",
    "\n",
    "print(wine_subset_scaled_df.head())\n",
    "print(\"\\n\")\n",
    "print(wine_subset_scaled_df.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been scaled and the variance is equal across columns.\n",
    "\n",
    "### scikit-learn refresher\n",
    "\n",
    "Let's do a quick refresher on k-nearest neighbors and scikit-learn in general, which we'll be using throughout the workshop. \n",
    "\n",
    "K-nearest neighbors is a model that classifies data based on its distance to training set data. A new data point is assigned a label based on the class that the majority of surrounding data points belongs to, using a distance metric like the Euclidean distance metric. \n",
    "\n",
    "Let's walk through an example of k-nearest neighbors and the scikit-learn workflow. First, let's generate a toy dataset using scikit-learn's `make_classification()` method, which generates a normally-distributed dataset you can use for classification problems. There are a number of ways you can customize the generated dataset, but we're going to keep it simple here. \n",
    "\n",
    "The parameters filled in below are:\n",
    "- `n_samples`: the number of samples (rows) in the dataset. The default is 100; here it's increased to 1000.\n",
    "- `n_features`: the number of features (columns) in the dataset. Instead of the default 20, let's keep it very simple at 3 features.\n",
    "- `n_classes`: the number of class labels. We'll keep it at the default 2. \n",
    "- `n_redundant`: the number of redundant features—features that aren't informative for classification—which is useful if you want to test out dimensionality reduction methods, for example. We're not doing that here, so we'll set it to 0.\n",
    "\n",
    "The function generates both the X and y sets, so we'll store those sets in the `X_gen` and `y_gen` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_gen, y_gen = make_classification(n_samples=1000, n_features=3, n_classes=2, n_redundant=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, let's build our model. We'll need both `train_test_split`, to split up our dataset before training, and `KNeighborsClassifier()`, to train the k-nearest neighbors model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split up our data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_gen, y_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've done that, we can create our `KNeighborsClassifier()` model, and then use `.fit()` to fit the model to our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step we want to take is to check the accuracy of our classifier. There are a multitude of ways you can validate the accuracy of your mode, but we'll just use `KNeighborsClassifier()`'s built in `.score()` method, which takes your `X` test data, predicts its `y` labels, and then compares the predicted `y` to the true values of `y` you pass in as the second parameter. The output is the percentage in the test `X` that received an accurate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.964"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've refreshed ourselves on the basics of k-nearest neighbors and scikit-learn, let's move on to applying standardization in a real modeling situation.\n",
    "\n",
    "### Wine_types modeling: non-scaled data\n",
    "\n",
    "Let's first take a look at the accuracy of a k-nearest neighbors model on the `wine_types` dataset without standardizing the data. We're going to use the features in this dataset to predict the `Type` of wine, of which there are three labels: 1, 2, and 3.\n",
    "\n",
    "Let's break out `Type` into its own `y_wine` dataset as our predicted class, and we'll keep the rest of the features in `X_wine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wine = wine_types.drop(\"Type\", axis=1)\n",
    "y_wine = wine_types[\"Type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already previously imported `train_test_split` so let's split up our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a new `KNeighborsClassifier()` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_wine_unscaled = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can fit the model to our training set and then take a look at its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6444444444444445"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_wine_unscaled.fit(X_train, y_train)\n",
    "\n",
    "knn_wine_unscaled.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy isn't terrible, but it's possible that we can improve our model by scaling our data!\n",
    "\n",
    "### Wine_types modeling: scaled data\n",
    "\n",
    "The accuracy score on the unscaled `wine_types` dataset was decent, but we can likely do better if we scale the dataset. The process is mostly the same as the previous exercise, with the added step of scaling the data.\n",
    "\n",
    "Let's apply the `StandardScaler` method to our `wine_types` feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_scaler = StandardScaler()\n",
    "\n",
    "X_scaled = wine_scaler.fit_transform(X_wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we'll split up the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(X_scaled, y_wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then fit the `KNeighborsClassifier` model to the data and take a look at the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_wine_scaled = KNeighborsClassifier()\n",
    "knn_wine_scaled.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "knn_wine_scaled.score(X_test_scaled, y_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there's been a dramatic increase in the accuracy of our model, simply by scaling our dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
